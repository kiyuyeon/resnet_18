{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8dad0c8-b5be-43b9-9670-d7b5aaf092b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jupyter-hong\n"
     ]
    }
   ],
   "source": [
    "import tarfile\n",
    "import os \n",
    "\n",
    "# Define the path to your tar.gz file\n",
    "os.chdir('/home/jupyter-choi/')\n",
    "print(os.getcwd())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0101c327-4812-4044-be5a-899ca30c7e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device_name = torch.cuda.get_device_name(4)\n",
    "    print(f\"GPU: {device_name}\")\n",
    "else:\n",
    "    print(\"No GPU available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9428f55-bd11-45ef-8ca3-51397fac10c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1629 .csv files in the folder.\n"
     ]
    }
   ],
   "source": [
    "# .npy 파일이 있는 폴더 경로\n",
    "folder_path = \"\"\n",
    "\n",
    "# 폴더 내의 모든 .npy 파일 로드\n",
    "file_list = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "# npy 파일의 개수\n",
    "num_npy_files = len(file_list)\n",
    "print(f\"There are {num_npy_files} .csv files in the folder.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcdbccc0-fc07-4621-8898-103a6f1c9366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "NVIDIA GeForce RTX 2080 Ti\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mset_device(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 모델과 데이터를 새 GPU로 이동\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# 데이터 로더 내의 데이터도 같은 방식으로 이동시켜야 합니다.\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 사용 가능한 GPU 확인\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "# GPU 변경 (예: 두 번째 GPU 사용)\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(1)\n",
    "\n",
    "# 모델과 데이터를 새 GPU로 이동\n",
    "model = model.to(device)\n",
    "# 데이터 로더 내의 데이터도 같은 방식으로 이동시켜야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b60fe93d-7cd2-4650-8b54-85b4c830e136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:3\n",
      "Unique labels: [0]\n",
      "Label distribution: [36868]\n",
      "Epoch 1, Train Loss: 0.0000, Train Accuracy: 100.00%\n",
      "Epoch 1, Validation Loss: 0.0000, Validation Accuracy: 100.00%\n",
      "Epoch 2, Train Loss: 0.0000, Train Accuracy: 100.00%\n",
      "Epoch 2, Validation Loss: 0.0000, Validation Accuracy: 100.00%\n",
      "Epoch 3, Train Loss: 0.0000, Train Accuracy: 100.00%\n",
      "Epoch 3, Validation Loss: 0.0000, Validation Accuracy: 100.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 194\u001b[0m\n\u001b[1;32m    191\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    192\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 194\u001b[0m train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(output\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    196\u001b[0m total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "def load_labels(label_file):\n",
    "    # 라벨 파일을 읽고, pid와 라벨을 매핑하는 딕셔너리를 생성합니다.\n",
    "    label_df = pd.read_csv(label_file)\n",
    "    label_dict = dict(zip(label_df['PID'], label_df['AF']))\n",
    "    return label_dict\n",
    "\n",
    "def preprocess_data(X):\n",
    "    # Z-점수 정규화를 수행합니다.\n",
    "    X_mean = X.mean(axis=0)\n",
    "    X_std = X.std(axis=0)\n",
    "    X_normalized = (X - X_mean) / X_std\n",
    "    return X_normalized\n",
    "\n",
    "def load_data(folder_path, label_dict):\n",
    "    X_list = []\n",
    "    y_list = []\n",
    "\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.npy'):\n",
    "            # 파일 이름에서 pid 추출\n",
    "            match = re.search(r's_(\\d+)_\\d+.npy', file_name)\n",
    "            if match:\n",
    "                pid = int(match.group(1))\n",
    "                \n",
    "                # 데이터 로드\n",
    "                data = np.load(os.path.join(folder_path, file_name))\n",
    "\n",
    "                # 데이터 형태 조정 (32채널을 1채널로 변경)\n",
    "                data = data.reshape(-1, 1, 2500)\n",
    "\n",
    "                # 라벨 할당\n",
    "                label = label_dict.get(pid, 0)  # pid가 없는 경우 기본값으로 0을 사용\n",
    "                X_list.append(data)\n",
    "                y_list.append(label)\n",
    "            else:\n",
    "                print(f\"파일 이름 형식이 맞지 않습니다: {file_name}\")\n",
    "\n",
    "    X = np.concatenate(X_list, axis=0)\n",
    "    y = np.array(y_list)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# 라벨 파일과 데이터 폴더 경로\n",
    "label_file = 'diffusion/mobile_data/12Lead+Mobile_ECG/orig_20230816/label/Total_Mobile_label12.csv'\n",
    "folder_path = 'diffusion/mobile_data/12Lead+Mobile_ECG/orig_20230816/data/Mobile_ECG_Scaled(-1,1)S'\n",
    "\n",
    "# 라벨 로드\n",
    "label_dict = load_labels(label_file)\n",
    "\n",
    "# 데이터 로드\n",
    "X, y = load_data(folder_path, label_dict)\n",
    "\n",
    "# 원본 데이터셋에서 훈련 및 검증 데이터셋, 테스트 데이터셋 분리\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 훈련 및 검증 데이터셋 전처리\n",
    "X_train_val_normalized = preprocess_data(X_train_val)\n",
    "X_train_val_tensor = torch.from_numpy(X_train_val_normalized).float()\n",
    "y_train_val_tensor = torch.from_numpy(y_train_val).long()\n",
    "\n",
    "# 데이터셋 생성 및 분할\n",
    "dataset = TensorDataset(X_train_val_tensor, y_train_val_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# 데이터 로더 생성\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "# ResNet 모델 정의\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != self.expansion * out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, self.expansion * out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm1d(self.expansion * out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        out += self.shortcut(identity)\n",
    "        out = F.relu(out)\n",
    "\n",
    "        out = F.dropout(out, p=0.5, training=self.training)  # 드롭아웃 추가\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=2):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = 64\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
    "        self.linear = None  # Remove the initialization from here\n",
    "\n",
    "    def _make_layer(self, block, out_channels, num_blocks, stride):\n",
    "        strides = [stride] + [1] * (num_blocks - 1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_channels, out_channels, stride))\n",
    "            self.in_channels = out_channels * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "\n",
    "        out = F.avg_pool1d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "\n",
    "        if self.linear is None:\n",
    "            self.linear = nn.Linear(out.size(1), self.num_classes).to(out.device)\n",
    "\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2], num_classes=len(np.unique(y)))\n",
    "\n",
    "# CUDA 사용 가능 여부 확인\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 데이터셋 및 레이블 분포 확인\n",
    "print(f\"Unique labels: {np.unique(y)}\")\n",
    "print(f\"Label distribution: {np.bincount(y)}\")\n",
    "\n",
    "\n",
    "# 모델, 손실 함수, 옵티마이저 초기화\n",
    "model = ResNet18().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "# 훈련 루프\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        # print(f\"Train batch output: {output[:5]}\")\n",
    "        \n",
    "    train_loss /= len(train_loader)\n",
    "    train_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "    # 검증 과정\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "    val_accuracy = 100 * correct / total\n",
    "    print(f'Epoch {epoch+1}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.2f}%')\n",
    "\n",
    "# 모델 저장\n",
    "torch.save(model.state_dict(), 'resnet_ecg_model.pth')\n",
    "\n",
    "# 테스트 데이터셋 전처리 및 DataLoader 생성\n",
    "X_test_normalized = preprocess_data(X_test)\n",
    "X_test_tensor = torch.from_numpy(X_test_normalized).float()\n",
    "y_test_tensor = torch.from_numpy(y_test).long()\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 테스트셋으로 예측 및 성능 평가\n",
    "model.eval()  # 모델을 평가 모드로 설정\n",
    "y_pred = []\n",
    "y_true = []\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        outputs = model(data)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        y_pred.extend(predicted.view(-1).cpu().numpy())\n",
    "        y_true.extend(target.view(-1).cpu().numpy())\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='macro')\n",
    "recall = recall_score(y_true, y_pred, average='macro')\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(f'Precision: {precision:.4f}')\n",
    "print(f'Recall: {recall:.4f}')\n",
    "print(f'F1 Score: {f1:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63f98533-712f-46ba-a44c-6adb2c8b8502",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33669</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33670</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33671</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33672</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33673</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33674 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0\n",
       "0      1.0\n",
       "1      1.0\n",
       "2      1.0\n",
       "3      1.0\n",
       "4      1.0\n",
       "...    ...\n",
       "33669  0.0\n",
       "33670  0.0\n",
       "33671  0.0\n",
       "33672  0.0\n",
       "33673  0.0\n",
       "\n",
       "[33674 rows x 1 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 파일 경로\n",
    "file_path = ''\n",
    "\n",
    "# 파일 로드\n",
    "data = np.load(file_path)\n",
    "\n",
    "# NumPy 배열을 Pandas DataFrame으로 변환\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# DataFrame 출력\n",
    "df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt20_py310",
   "language": "python",
   "name": "pt20_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
